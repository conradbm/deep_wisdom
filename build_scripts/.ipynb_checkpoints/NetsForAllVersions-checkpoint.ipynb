{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/laurensuarez/Desktop/deep_wisdom/deep_wisdom_django/dwsite/data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/Users/laurensuarez/Desktop/deep_wisdom/deep_wisdom_django/dwsite/data\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get version data conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurensuarez/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New International Version 31086\n",
      "New Living Translation 31086\n",
      "English Standard Version 31086\n",
      "Berean Study Bible 31086\n",
      "New American Standard Bible  31102\n",
      "King James Bible 31102\n",
      "Christian Standard Bible 31084\n",
      "Contemporary English Version 27863\n",
      "Good News Translation 29807\n",
      "Holman Christian Standard Bible 31101\n",
      "International Standard Version 31096\n",
      "NET Bible 31086\n",
      "New Heart English Bible 31090\n",
      "GOD'S WORDÂ® Translation 31087\n",
      "JPS Tanakh 1917 23145\n",
      "New American Standard 1977  31102\n",
      "Jubilee Bible 2000 31102\n",
      "King James 2000 Bible 31102\n",
      "American King James Version 31102\n",
      "American Standard Version 31100\n",
      "Brenton Septuagint Translation 22830\n",
      "Douay-Rheims Bible 31102\n",
      "Darby Bible Translation 31099\n",
      "English Revised Version 31102\n",
      "Webster's Bible Translation 31102\n",
      "World English Bible 31098\n",
      "Young's Literal Translation 31102\n",
      "Aramaic Bible in Plain English 11333\n",
      "Berean Literal Bible 7941\n",
      "Weymouth New Testament 7957\n",
      "New Living Translation\n",
      "10000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import pickle\n",
    "import numpy as np\n",
    "import keras\n",
    "import nltk\n",
    "import string\n",
    "import pickle\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "#nltk.download('punkt')\n",
    "\n",
    "class MacOSFile(object):\n",
    "\n",
    "    def __init__(self, f):\n",
    "        self.f = f\n",
    "\n",
    "    def __getattr__(self, item):\n",
    "        return getattr(self.f, item)\n",
    "\n",
    "    def read(self, n):\n",
    "        # print(\"reading total_bytes=%s\" % n, flush=True)\n",
    "        if n >= (1 << 31):\n",
    "            buffer = bytearray(n)\n",
    "            idx = 0\n",
    "            while idx < n:\n",
    "                batch_size = min(n - idx, 1 << 31 - 1)\n",
    "                # print(\"reading bytes [%s,%s)...\" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "                buffer[idx:idx + batch_size] = self.f.read(batch_size)\n",
    "                # print(\"done.\", flush=True)\n",
    "                idx += batch_size\n",
    "            return buffer\n",
    "        return self.f.read(n)\n",
    "\n",
    "    def write(self, buffer):\n",
    "        n = len(buffer)\n",
    "        print(\"writing total_bytes=%s...\" % n, flush=True)\n",
    "        idx = 0\n",
    "        while idx < n:\n",
    "            batch_size = min(n - idx, 1 << 31 - 1)\n",
    "            print(\"writing bytes [%s, %s)... \" % (idx, idx + batch_size), end=\"\", flush=True)\n",
    "            self.f.write(buffer[idx:idx + batch_size])\n",
    "            print(\"done.\", flush=True)\n",
    "            idx += batch_size\n",
    "\n",
    "def pickle_dump(obj, file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        return pickle.dump(obj, MacOSFile(f), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def pickle_load(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        return pickle.load(MacOSFile(f))\n",
    "\n",
    "\n",
    "### GET DATA\n",
    "with open(\"bible_data_20181129_update.pkl\", \"rb\") as handle:\n",
    "    bible_data=pickle.load(handle)\n",
    "\n",
    "### DISPLAY DIFFERENCES\n",
    "for k,v in bible_data.items():\n",
    "    print(\"{} {}\".format(k, len(v.items())))\n",
    "\n",
    "\n",
    "### CONSTRUCT EACH ENGINE\n",
    "int2verse=0\n",
    "verse2int=0\n",
    "force=False\n",
    "version_bible_data=0\n",
    "for version,versionText in bible_data.items():\n",
    "    version_formatted = version.replace(\" \", \"_\")\n",
    "    if not os.path.exists(version_formatted):\n",
    "        os.mkdir(\"{}\".format(version_formatted))\n",
    "        os.chdir(version_formatted)\n",
    "    else:\n",
    "        if force:\n",
    "            pass\n",
    "        else:\n",
    "            continue\n",
    "    print(version)\n",
    "    version_bible_data=versionText\n",
    "\n",
    "    int2verse={}\n",
    "    verse2int={}\n",
    "    for i,v in enumerate(version_bible_data.keys()):\n",
    "        int2verse[i]=v\n",
    "        verse2int[v]=i\n",
    "    #int2verse\n",
    "    #verse2int\n",
    "\n",
    "    bible_not_found_cf=set()\n",
    "    version_bible_mapping={}\n",
    "    num_verses=len(int2verse.keys())\n",
    "    i=0\n",
    "    for k,obj in version_bible_data.items():\n",
    "        #print(k, obj[0], obj[1])\n",
    "        mapping=np.zeros((num_verses))\n",
    "        for cf in obj[1]:\n",
    "            try:\n",
    "                mapping[verse2int[cf]]=1\n",
    "            except Exception as e:\n",
    "                bible_not_found_cf.add(cf)\n",
    "        version_bible_mapping[k]=[obj[0],mapping]\n",
    "        i+=1\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "    #kjv_bible_mapping[\"Genesis 1:1\"][1]\n",
    "    \n",
    "    \"\"\" Uncomment for production saving ...\n",
    "    \"\"\"\n",
    "    pickle_dump(version_bible_mapping, \"{}_bible_mapping.pkl\".format(version_formatted))\n",
    "    pickle_dump(int2verse, \"{}_int2verse_mapping.pkl\".format(version_formatted))\n",
    "    pickle_dump(verse2int, \"{}_verse2int_mapping.pkl\".format(version_formatted))\n",
    "    print(\"Successful saving {}.\".format(version_formatted))\n",
    "    print(\"Lost {} verses for some reason.\".format(bible_not_found_cf))\n",
    "    \n",
    "    \n",
    "    print(\"Getting Corpus\")\n",
    "    corpus=list(map(lambda x:x[1][0], version_bible_mapping.items()))\n",
    "    print(corpus[0])\n",
    "\n",
    "    print(\"TFIDF Vectorizing\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(#tokenizer=tokenize, \n",
    "                                       stop_words=\"english\",\n",
    "                                       lowercase=True, \n",
    "                                       min_df=0.0001,\n",
    "                                       max_df=0.9999)\n",
    "    print(\"Length of corpus: {}\".format(len(corpus)))\n",
    "    tfidf_fit=tfidf_vectorizer.fit(corpus)\n",
    "    pickle_dump(tfidf_fit, \"{}_tfidf_fit.pkl\".format(version_formatted))\n",
    "    tfidf_mat=tfidf_vectorizer.transform(corpus).todense()\n",
    "    X=tfidf_mat\n",
    "    y=np.array(list(map(lambda x:x[1][1], version_bible_mapping.items())))\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(len(corpus))\n",
    "    print(len(list(version_bible_mapping.keys())))\n",
    "\n",
    "    print(\"Building Model\")\n",
    "    from keras.layers import Dense, Dropout, Conv1D, MaxPooling1D\n",
    "    from keras import metrics\n",
    "    import os\n",
    "    import datetime\n",
    "    def create_model(X,y):\n",
    "        # Input layers\n",
    "        print(X.shape)\n",
    "        print(y.shape)\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10000, input_shape=(X.shape[1],)))\n",
    "        model.add(Dense(1000))\n",
    "        model.add(Dense(100))\n",
    "        model.add(Dense(y.shape[1], activation='softmax'))\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['categorical_accuracy'])\n",
    "        return model\n",
    "    \n",
    "    #model_path=\"weights-improvement-01-54.2576.hdf5\"\n",
    "    def load_trained_model(weights_path, X, y):\n",
    "        model = create_model(X,y)\n",
    "        model.load_weights(weights_path)\n",
    "        print(\"Loaded\")\n",
    "        return model\n",
    "\n",
    "    #model=load_trained_model(model_path,X,y)\n",
    "\n",
    "    filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model = create_model(X,y)\n",
    "    model.summary()\n",
    "\n",
    "    model.fit(X, y,\n",
    "                  batch_size=128,\n",
    "                  epochs=1,\n",
    "                  callbacks=callbacks_list)\n",
    "    os.chdir('..')\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Guide me in your truth and teach me, for you are God my Savior, and my hope is in you all day long.',\n",
       " ['Genesis 49:18',\n",
       "  '1 Kings 8:36',\n",
       "  'Psalm 1:2',\n",
       "  'Psalm 25:10',\n",
       "  'Psalm 40:1',\n",
       "  'Psalm 43:3',\n",
       "  'Psalm 51:14',\n",
       "  'Psalm 79:9',\n",
       "  'Psalm 86:3',\n",
       "  'Psalm 86:11',\n",
       "  'Psalm 143:10',\n",
       "  'Isaiah 26:7',\n",
       "  'Habakkuk 3:18'],\n",
       " '/psalms/25-5.htm']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect=tfidf_fit.transform([\"love\"]).todense()\n",
    "\n",
    "print(model.predict(vect).argsort()[0][0])\n",
    "i=model.predict(vect).argsort()[0][-50]\n",
    "versionText[int2verse[i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Vectorizing\n",
      "Length of corpus: 31086\n"
     ]
    }
   ],
   "source": [
    "print(\"TFIDF Vectorizing\")\n",
    "tfidf_vectorizer = TfidfVectorizer(#tokenizer=tokenize, \n",
    "                                   stop_words=\"english\",\n",
    "                                   lowercase=True, \n",
    "                                   min_df=0.0001,\n",
    "                                   max_df=0.9999)\n",
    "print(\"Length of corpus: {}\".format(len(corpus)))\n",
    "tfidf_fit=tfidf_vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_fit.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New International Version 31086\n",
      "New Living Translation 31086\n",
      "English Standard Version 31086\n",
      "Berean Study Bible 31086\n",
      "New American Standard Bible  31102\n",
      "King James Bible 31102\n",
      "Christian Standard Bible 31084\n",
      "Contemporary English Version 27863\n",
      "Good News Translation 29807\n",
      "Holman Christian Standard Bible 31101\n",
      "International Standard Version 31096\n",
      "NET Bible 31086\n",
      "New Heart English Bible 31090\n",
      "GOD'S WORDÂ® Translation 31087\n",
      "JPS Tanakh 1917 23145\n",
      "New American Standard 1977  31102\n",
      "Jubilee Bible 2000 31102\n",
      "King James 2000 Bible 31102\n",
      "American King James Version 31102\n",
      "American Standard Version 31100\n",
      "Brenton Septuagint Translation 22830\n",
      "Douay-Rheims Bible 31102\n",
      "Darby Bible Translation 31099\n",
      "English Revised Version 31102\n",
      "Webster's Bible Translation 31102\n",
      "World English Bible 31098\n",
      "Young's Literal Translation 31102\n",
      "Aramaic Bible in Plain English 11333\n",
      "Berean Literal Bible 7941\n",
      "Weymouth New Testament 7957\n"
     ]
    }
   ],
   "source": [
    "for k,v in bible_data.items():\n",
    "    print(\"{} {}\".format(k, len(v.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
